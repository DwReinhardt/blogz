"""
Resume Bullet Analyzer
Purpose
    This tool automates the analysis and organization of resume bullets from multiple tailored resume versions. It solves a common problem for job seekers who maintain dozens of customized resumes: how to consolidate, deduplicate, and organize hundreds of bullet points into a master library that's easy to search and reuse.
    The analyzer produces a 3-tier system:

    Tier 1: Your top 12 "greatest hits" bullets with highest impact
    Tier 2: Complete master library organized by company/role
    Tier 3: Cross-reference by competency area (e.g., "Strategic Planning," "Workforce Management")

This enables you to quickly pull relevant bullets when tailoring resumes for specific job applications, ensuring you always use the most impactful version of each accomplishment.

Key Features
    ✅ Intelligent Deduplication: When the same accomplishment appears multiple times with different metrics (e.g., "20% improvement" vs "25% improvement"), automatically keeps the version with the highest numbers
    ✅ Metric Extraction: Identifies and scores percentages, dollar amounts, team sizes, and other quantitative achievements
    ✅ Auto-Categorization: Classifies bullets into 17+ competency areas (Leadership, Financial Management, Training & Development, etc.)
    ✅ Impact Scoring: Ranks bullets based on metrics, action verbs, and scope of responsibility
    ✅ Company Normalization: Handles variations in company names and keeps divisions separate when appropriate
    ✅ Markdown Export: Generates clean, formatted output ready to convert to Word/PDF

Usage
Basic Usage
    python from resume_analyzer import ResumeBulletAnalyzer

    # Initialize with your data file
        analyzer = ResumeBulletAnalyzer("Collated_Bullets.txt")

    # Export complete 3-tier analysis to markdown
        analyzer.export_to_markdown("Resume_Bullet_Library.md")

Advanced Usage
    # Get deduplicated dataframe for custom analysis
        deduped_df = analyzer.deduplicate_bullets()

    # Generate individual tiers
        tier1 = analyzer.generate_tier1_greatest_hits(deduped_df, n=15)  # Top 15 instead of 12
        tier2 = analyzer.generate_tier2_by_company(deduped_df)
        tier3 = analyzer.generate_tier3_competency_cross_ref(deduped_df)

    # Get statistics
        stats = analyzer.generate_company_statistics(deduped_df)
        print(f"Analyzed {stats['Wells Fargo']['total_bullets']} bullets from Wells Fargo")

Input File Format
    Your input file should be a CSV/TSV with these columns:
        Role:       Job title
        Company:    Company name
        Location:   City, State
        Dates:      Date range
        Bullet:     The bullet point text
        SourceFile: (Optional) Which resume version this came from

Dependencies
    Required Python Packages
        pip install pandas
    
    Standard Library (No Installation Needed)
        re - Regular expressions for metric extraction
        collections.defaultdict - Data organization
        typing - Type hints

    Python Version
        Python 3.7+ (uses f-strings and type hints)

Output Format
    The tool generates a markdown file with three main sections:
    Tier 1: Greatest Hits (12 bullets)
        The highest-impact bullets across your entire career, grouped by theme (Strategic Leadership, Operations, Talent Management, etc.). Copy-paste ready for immediate use.

    Tier 2: Master Library by Company
        All bullets organized by employer and competency area. Complete reference showing everything you've accomplished at each role.

    Tier  3: Competency Cross-Reference
        Bullets reorganized by skill area (e.g., all your "Financial Management" bullets from all companies in one place). Perfect for quickly finding relevant bullets for specific job requirements.

    Statistics Summary: Shows bullet counts, top competencies, and key metrics per company.

Customization
    Adjust Impact Scoring
    Modify calculate_impact_score() to weight different factors:
        python
        def calculate_impact_score(self, row: pd.Series) -> float:
            score = 0
            # Weight percentages more heavily
            for metric in row['Metrics']:
                if metric['type'] == 'percentage':
                    val = float(metric['value'].replace('%', ''))
                    score += val * 5  # Changed from 2 to 5
        return score
    
    Add/Modify Competency Categories
    Edit the competency_keywords dictionary in categorize_by_competency():
        python
        competency_keywords = {
            'Strategic Planning': ['strategic', 'planning', 'roadmap'],
            'Your New Category': ['keyword1', 'keyword2', 'keyword3'],
            # ... add more
        }
    
    Change Company Normalization Rules
    Update normalize_company() to handle your specific employers:
        python
        def normalize_company(self, company: str) -> str:
            if 'Google' in company:
                return 'Google LLC'
        # Add your rules here

Limitations
    ⚠️ Metric Extraction: Works well for common formats (20%, $5M, 4,000 employees) but may miss unconventional metrics
    ⚠️ Semantic Similarity: Doesn't detect paraphrased duplicates (e.g., "reduced costs" vs "cut expenses") - only exact/near-exact text matches
    ⚠️ Context Understanding: Auto-categorization uses keywords, not true semantic understanding
    ⚠️ Manual Review Recommended: While 85-90% accurate, human review improves the final output

Tips for Best Results
    Standardize Input:  Use consistent formatting in your source resumes (same bullet style, metric formats)
    Review Tier 1:      The algorithm ranks by metrics, but you may want to manually adjust based on relevance to your target roles
    Keep Source Files:  The SourceFile column helps you remember which bullets worked for which companies/roles
    Update Regularly:   Rerun the analysis whenever you add new accomplishments or tailor more resumes
    Export to Word:     Convert the markdown output to .docx for easier formatting and sharing

Troubleshooting
    Problem: Bullets not deduplicating
    Solution: Check for extra spaces, punctuation differences. The tool looks for exact text matches.

    Problem: Metrics not extracting correctly
    Solution: Ensure metrics use standard formats: 20%, $5M, 4,000 employees

    Problem: Categories seem wrong
    Solution: Customize the competency_keywords dictionary for your industry

    Problem: CSV parsing errors
    Solution: Ensure proper encoding (UTF-8) and escaped commas within bullet text

License
    MIT License - Feel free to modify and use for personal or commercial purposes.

Contributing
    Improvements welcome! Priority areas:
        Better semantic deduplication (using NLP/embeddings)
        More sophisticated metric extraction
        Web interface for non-technical users
        Export to other formats (JSON, Excel, PDF)




"""
import pandas as pd
import re
from collections import defaultdict
from typing import Dict, List, Tuple

class ResumeBulletAnalyzer:
    """
    Analyzes resume bullets from a CSV/Excel file and produces:
    - Tier 1: Greatest Hits (top bullets)
    - Tier 2: Master library by company
    - Tier 3: Cross-reference by competency
    """
    
    def __init__(self, file_path: str):
        """Initialize with path to collated bullets file"""
        self.df = pd.read_csv(file_path, encoding='utf-8')
        self.clean_data()
        
    def clean_data(self):
        """Clean and normalize the data"""
        # Remove duplicates based on bullet text similarity
        self.df['Bullet_Clean'] = self.df['Bullet'].str.strip()
        
        # Normalize company names
        self.df['Company_Normalized'] = self.df['Company'].apply(self.normalize_company)
        
        # Extract metrics from bullets
        self.df['Metrics'] = self.df['Bullet'].apply(self.extract_metrics)
        
    def normalize_company(self, company: str) -> str:
        """Normalize company names"""
        company = str(company).strip()
        
        # Keep Wells Fargo divisions separate
        if 'Wells Fargo' in company and 'Technology' in company:
            return 'Wells Fargo - Technology Division'
        elif 'Wells Fargo' in company and 'CRMS' in company:
            return 'Wells Fargo - CRMS Strategy & Analytics'
        
        # Keep USS ships separate
        if 'USS ENTERPRISE' in company:
            return 'USS ENTERPRISE (CVN 65)'
        elif 'USS HARRY' in company:
            return 'USS HARRY S TRUMAN (CVN 75)'
        
        # Normalize other names
        if 'Special Operations' in company or 'USSOCOM' in company:
            return 'U.S. Special Operations Command'
        elif 'Booz Allen' in company:
            return 'Booz Allen Hamilton'
        elif 'Naval Information' in company:
            return 'U.S. Naval Information Forces'
        elif 'Training Center' in company:
            return 'Navy and Marine Corps Intelligence Training Center'
        elif 'Capella' in company:
            return 'Capella University'
        
        return company
    
    def extract_metrics(self, bullet: str) -> List[Dict]:
        """Extract quantitative metrics from bullet text"""
        metrics = []
        
        # Pattern for percentages
        pct_pattern = r'(\d+(?:\.\d+)?%)'
        percentages = re.findall(pct_pattern, bullet)
        
        # Pattern for dollar amounts
        dollar_pattern = r'\$(\d+(?:\.\d+)?[MBK]?)'
        dollars = re.findall(dollar_pattern, bullet)
        
        # Pattern for numbers with context
        num_pattern = r'(\d+(?:,\d+)*(?:\+)?)\s+(\w+)'
        numbers = re.findall(num_pattern, bullet)
        
        for pct in percentages:
            metrics.append({'type': 'percentage', 'value': pct})
        
        for dollar in dollars:
            metrics.append({'type': 'currency', 'value': f'${dollar}'})
        
        for num, context in numbers:
            metrics.append({'type': 'quantity', 'value': num, 'context': context})
        
        return metrics
    
    def deduplicate_bullets(self) -> pd.DataFrame:
        """
        Deduplicate bullets by keeping highest metrics
        when same bullet appears multiple times
        """
        # Group by normalized bullet text
        grouped = self.df.groupby('Bullet_Clean')
        
        deduped = []
        
        for bullet_text, group in grouped:
            if len(group) == 1:
                deduped.append(group.iloc[0])
            else:
                # Multiple versions - pick one with highest metrics
                best_row = self.select_best_version(group)
                deduped.append(best_row)
        
        return pd.DataFrame(deduped)
    
    def select_best_version(self, group: pd.DataFrame) -> pd.Series:
        """Select the version with highest/best metrics"""
        
        # Score each row based on metrics
        scores = []
        for idx, row in group.iterrows():
            score = 0
            metrics = row['Metrics']
            
            for metric in metrics:
                if metric['type'] == 'percentage':
                    # Extract numeric value
                    val = float(metric['value'].replace('%', ''))
                    score += val
                elif metric['type'] == 'currency':
                    # Weight currency highly
                    val_str = metric['value'].replace('$', '')
                    multiplier = 1
                    if 'M' in val_str:
                        multiplier = 1_000_000
                        val_str = val_str.replace('M', '')
                    elif 'B' in val_str:
                        multiplier = 1_000_000_000
                        val_str = val_str.replace('B', '')
                    elif 'K' in val_str:
                        multiplier = 1_000
                        val_str = val_str.replace('K', '')
                    
                    try:
                        score += float(val_str) * multiplier / 100000  # Normalize
                    except:
                        pass
            
            scores.append(score)
        
        # Return row with highest score
        best_idx = scores.index(max(scores))
        return group.iloc[best_idx]
    
    def categorize_by_competency(self, bullet: str) -> List[str]:
        """Categorize bullets by competency area"""
        competencies = []
        
        bullet_lower = bullet.lower()
        
        # Define competency keywords
        competency_keywords = {
            'Strategic Planning': ['strategic', 'planning', 'roadmap', 'vision', 'okr', 'kpi', 'initiative'],
            'Workforce Planning': ['workforce', 'headcount', 'hiring', 'staffing', 'talent acquisition', 'recruitment'],
            'Change Management': ['change management', 'transformation', 'telework', 'remote work', 'transition', 'restructur'],
            'Compensation & Job Architecture': ['compensation', 'job titling', 'job architecture', 'market benchmark', 'pay'],
            'Employee Engagement': ['engagement', 'satisfaction', 'retention', 'turnover', 'survey', 'morale'],
            'Performance Management': ['performance', 'okr', 'kpi', 'metrics', 'goals', 'review'],
            'Financial Management': ['budget', 'financial', 'forecast', 'cost savings', 'finance', '$'],
            'Diversity & Inclusion': ['diversity', 'inclusion', 'dei', 'underrepresented', 'affirmative action'],
            'Training & Development': ['training', 'development', 'learning', 'upskilling', 'curriculum', 'instructional'],
            'Data Science & Analytics': ['data science', 'analytics', 'dashboard', 'power bi', 'hris', 'data'],
            'Program Management': ['program', 'project', 'portfolio', 'delivery', 'implementation'],
            'Communication': ['communication', 'stakeholder', 'messaging', 'announcement', 'collaboration'],
            'Process Improvement': ['process', 'efficiency', 'streamline', 'optimization', 'lean', 'agile'],
            'HR Systems': ['hris', 'workday', 'peoplesoft', 'hr system', 'hr technology'],
            'Leadership': ['led', 'directed', 'managed team', 'supervised', 'chief of staff'],
            'Crisis Management': ['crisis', 'pandemic', 'covid', 'emergency', 'business continuity'],
            'Onboarding': ['onboarding', 'new hire', 'orientation', 'integration'],
        }
        
        for competency, keywords in competency_keywords.items():
            if any(keyword in bullet_lower for keyword in keywords):
                competencies.append(competency)
        
        return competencies if competencies else ['General']
    
    def generate_tier1_greatest_hits(self, deduped_df: pd.DataFrame, n: int = 12) -> List[Dict]:
        """Generate Tier 1: Greatest Hits bullets"""
        
        # Score bullets based on impact
        deduped_df['Impact_Score'] = deduped_df.apply(self.calculate_impact_score, axis=1)
        
        # Sort by impact score
        top_bullets = deduped_df.nlargest(n, 'Impact_Score')
        
        # Group into categories
        greatest_hits = []
        for _, row in top_bullets.iterrows():
            greatest_hits.append({
                'bullet': row['Bullet'],
                'company': row['Company_Normalized'],
                'competencies': self.categorize_by_competency(row['Bullet']),
                'score': row['Impact_Score']
            })
        
        return greatest_hits
    
    def calculate_impact_score(self, row: pd.Series) -> float:
        """Calculate impact score for a bullet"""
        score = 0
        
        # Base score on metrics
        for metric in row['Metrics']:
            if metric['type'] == 'percentage':
                val = float(metric['value'].replace('%', ''))
                score += val * 2  # Weight percentages highly
            elif metric['type'] == 'currency':
                score += 50  # High base score for any dollar amount
        
        # Bonus for certain keywords
        bullet_lower = row['Bullet'].lower()
        
        high_value_keywords = ['led', 'directed', 'managed', 'established', 'drove', 
                               'spearheaded', 'transformed', 'achieved']
        
        for keyword in high_value_keywords:
            if keyword in bullet_lower:
                score += 10
        
        # Bonus for team size mentions
        if any(str(x) in row['Bullet'] for x in ['4,000', '4000', '1,500', '1500', '600+']):
            score += 20
        
        return score
    
    def generate_tier2_by_company(self, deduped_df: pd.DataFrame) -> Dict:
        """Generate Tier 2: Master library organized by company"""
        
        company_bullets = defaultdict(lambda: defaultdict(list))
        
        for _, row in deduped_df.iterrows():
            company = row['Company_Normalized']
            competencies = self.categorize_by_competency(row['Bullet'])
            
            # Add to each relevant competency
            for comp in competencies:
                company_bullets[company][comp].append(row['Bullet'])
        
        return dict(company_bullets)
    
    def generate_tier3_competency_cross_ref(self, deduped_df: pd.DataFrame) -> Dict:
        """Generate Tier 3: Cross-reference by competency"""
        
        competency_map = defaultdict(lambda: {'companies': set(), 'bullets': []})
        
        for _, row in deduped_df.iterrows():
            competencies = self.categorize_by_competency(row['Bullet'])
            company = row['Company_Normalized']
            
            for comp in competencies:
                competency_map[comp]['companies'].add(company)
                competency_map[comp]['bullets'].append({
                    'text': row['Bullet'],
                    'company': company
                })
        
        # Convert sets to lists for JSON serialization
        for comp in competency_map:
            competency_map[comp]['companies'] = list(competency_map[comp]['companies'])
        
        return dict(competency_map)
    
    def generate_company_statistics(self, deduped_df: pd.DataFrame) -> Dict:
        """Generate summary statistics by company"""
        
        stats = {}
        
        for company in deduped_df['Company_Normalized'].unique():
            company_df = deduped_df[deduped_df['Company_Normalized'] == company]
            
            # Count bullets
            total_bullets = len(company_df)
            
            # Count by competency
            competency_counts = defaultdict(int)
            for _, row in company_df.iterrows():
                comps = self.categorize_by_competency(row['Bullet'])
                for comp in comps:
                    competency_counts[comp] += 1
            
            # Get top competencies
            top_comps = sorted(competency_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            
            # Extract key metrics
            all_metrics = []
            for _, row in company_df.iterrows():
                all_metrics.extend(row['Metrics'])
            
            stats[company] = {
                'total_bullets': total_bullets,
                'top_competencies': top_comps,
                'key_metrics': self.extract_key_metrics(company_df)
            }
        
        return stats
    
    def extract_key_metrics(self, company_df: pd.DataFrame) -> List[str]:
        """Extract key metrics from a company's bullets"""
        key_metrics = []
        
        for _, row in company_df.iterrows():
            bullet = row['Bullet']
            
            # Look for high-impact metrics
            if any(x in bullet for x in ['$', 'M', 'B']):
                # Extract dollar amounts
                dollar_matches = re.findall(r'\$\d+(?:\.\d+)?[MBK]?', bullet)
                key_metrics.extend(dollar_matches)
            
            # Look for team sizes
            team_matches = re.findall(r'(\d{1,3}(?:,\d{3})*\+?)\s*(?:person|employee|member|customer)', bullet, re.IGNORECASE)
            key_metrics.extend([f"{m[0]} people" for m in team_matches])
            
            # Look for high percentages
            pct_matches = re.findall(r'(\d{2,3}(?:\.\d+)?%)', bullet)
            key_metrics.extend(pct_matches)
        
        # Return unique, sorted metrics
        return sorted(list(set(key_metrics)))[:5]
    
    def export_to_markdown(self, output_file: str):
        """Export all tiers to a markdown file"""
        
        # Deduplicate
        deduped_df = self.deduplicate_bullets()
        
        # Generate all tiers
        tier1 = self.generate_tier1_greatest_hits(deduped_df)
        tier2 = self.generate_tier2_by_company(deduped_df)
        tier3 = self.generate_tier3_competency_cross_ref(deduped_df)
        stats = self.generate_company_statistics(deduped_df)
        
        # Write to markdown
        with open(output_file, 'w', encoding='utf-8') as f:
            # Header
            f.write("# Professional Experience Bullet Library - 3 Tier System\n\n")
            
            # Tier 1
            f.write("## TIER 1: Greatest Hits (Top 12 Bullets)\n")
            f.write("*Highest-impact, most versatile bullets for immediate resume use*\n\n")
            
            # Group by competency for Tier 1
            tier1_by_comp = defaultdict(list)
            for item in tier1:
                primary_comp = item['competencies'][0] if item['competencies'] else 'General'
                tier1_by_comp[primary_comp].append(item)
            
            bullet_num = 1
            for comp, items in tier1_by_comp.items():
                f.write(f"### {comp}\n")
                for item in items:
                    f.write(f"{bullet_num}. **{item['bullet']}**\n\n")
                    bullet_num += 1
            
            f.write("---\n\n")
            
            # Tier 2
            f.write("## TIER 2: Master Experience Library by Company\n\n")
            
            for company, competencies in sorted(tier2.items()):
                f.write(f"### **{company}**\n\n")
                
                for comp, bullets in sorted(competencies.items()):
                    if bullets:
                        f.write(f"#### {comp}\n")
                        for bullet in bullets:
                            f.write(f"- {bullet}\n")
                        f.write("\n")
                
                f.write("---\n\n")
            
            # Tier 3
            f.write("## TIER 3: Cross-Reference by Competency Area\n\n")
            
            for comp, data in sorted(tier3.items()):
                f.write(f"### **{comp}**\n")
                f.write(f"**Companies:** {', '.join(sorted(data['companies']))}\n\n")
                f.write("**Key Bullets:**\n")
                
                # Show top 5 bullets for this competency
                for i, bullet_data in enumerate(data['bullets'][:5]):
                    f.write(f"- {bullet_data['text']} ({bullet_data['company']})\n")
                
                f.write("\n---\n\n")
            
            # Statistics
            f.write("## Summary Statistics by Company\n\n")
            
            for company, stat in sorted(stats.items()):
                f.write(f"### {company}\n")
                f.write(f"- **Total Unique Bullets:** {stat['total_bullets']}\n")
                
                if stat['top_competencies']:
                    top_comp_str = ', '.join([f"{comp} ({count})" for comp, count in stat['top_competencies']])
                    f.write(f"- **Top Competencies:** {top_comp_str}\n")
                
                if stat['key_metrics']:
                    metrics_str = ', '.join(stat['key_metrics'])
                    f.write(f"- **Highest Impact Metrics:** {metrics_str}\n")
                
                f.write("\n")
        
        print(f"✅ Analysis complete! Output saved to: {output_file}")


# Usage Example
if __name__ == "__main__":
    # Initialize analyzer
    analyzer = ResumeBulletAnalyzer("Collated Bullets.txt")
    
    # Export to markdown
    analyzer.export_to_markdown("Resume_Bullet_Library_Complete.md")
    
    # You can also access individual components:
    deduped_df = analyzer.deduplicate_bullets()
    
    tier1 = analyzer.generate_tier1_greatest_hits(deduped_df, n=12)
    print(f"\nGenerated {len(tier1)} greatest hits bullets")
    
    tier2 = analyzer.generate_tier2_by_company(deduped_df)
    print(f"Organized bullets across {len(tier2)} companies")
    
    tier3 = analyzer.generate_tier3_competency_cross_ref(deduped_df)
    print(f"Identified {len(tier3)} competency areas")
    
    stats = analyzer.generate_company_statistics(deduped_df)
    print(f"Generated statistics for {len(stats)} companies")